{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vH4wc4iD_6w_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741538890157,"user_tz":300,"elapsed":14813,"user":{"displayName":"Saniya Abushakimova","userId":"05417858338122514786"}},"outputId":"64224863-3ecd-40b9-87d9-2447c6fd5b95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpNsPHZc_879","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741538890841,"user_tz":300,"elapsed":681,"user":{"displayName":"Saniya Abushakimova","userId":"05417858338122514786"}},"outputId":"d1ad4a36-5add-4380-beec-778152f0ac58"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/CS_444/CS_444_MPs/MP2/assignment2_starter_code\n"]}],"source":["import os\n","datadir = \"path/\"\n","if not os.path.exists(datadir):\n","  !ln -s \"path/\" $datadir\n","os.chdir(datadir)\n","!pwd"]},{"cell_type":"markdown","metadata":{"id":"2cHqo6b1_Bzk"},"source":["# Implement a Neural Network\n","\n","This project includes a custom implementation of a fully connected neural network, developed from scratch using NumPy. The core logic—including the forward pass, backward propagation, and parameter updates—is defined in `models/neural_net.py`.\n","\n","The neural network is structured as a class, NeuralNetwork, where all learnable parameters are stored in the self.params dictionary. Keys represent layer-specific parameter names, and values are NumPy arrays containing weights and biases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTt_CiWh_Bzm"},"outputs":[],"source":["import numpy as np\n","\n","from models.neural_net import NeuralNetwork\n","\n","# For auto-reloading external modules\n","# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\"Returns relative error\"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","metadata":{"id":"b5X9DO-5_Bzn"},"source":["The cell below initializes a toy dataset and corresponding model which will allow to check forward and backward pass by using a numeric gradient check."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"358jAXcc_Bzn"},"outputs":[],"source":["input_size = 2\n","hidden_size = 10\n","num_classes = 3\n","num_inputs = 5\n","optimizer = 'SGD'\n","\n","\n","def init_toy_model(num_layers):\n","    \"\"\"Initializes a toy model\"\"\"\n","    np.random.seed(0)\n","    hidden_sizes = [hidden_size] * (num_layers - 1)\n","    return NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, optimizer)\n","\n","def init_toy_data():\n","    \"\"\"Initializes a toy dataset\"\"\"\n","    np.random.seed(0)\n","    X = np.random.randn(num_inputs, input_size)\n","    y = np.random.randn(num_inputs, num_classes)\n","    return X, y\n"]},{"cell_type":"markdown","metadata":{"id":"GjAwpT2z_Bzo"},"source":["# Gradient  check\n","\n","To ensure the correctness of the backward pass, this project includes a numerical gradient check. This method compares the analytical gradients computed via backpropagation with numerical approximations.\n","\n","If backward pass is implemented correctly, the maximum relative error between the analytical and numerical gradients should be around 1e-7 or lower for all parameters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZM47qUP_Bzo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741539893004,"user_tz":300,"elapsed":719,"user":{"displayName":"Saniya Abushakimova","userId":"05417858338122514786"}},"outputId":"d7fa3dac-90a0-4505-bb70-2024af9a87eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["W1 max relative error: 6.713448e-09\n","b1 max relative error: 4.281521e-10\n","W2 max relative error: 6.396117e-09\n","b2 max relative error: 6.769684e-10\n","W1 max relative error: 1.124219e-08\n","b1 max relative error: 3.926769e-09\n","W2 max relative error: 2.494194e-08\n","b2 max relative error: 8.426716e-10\n","W3 max relative error: 7.354338e-10\n","b3 max relative error: 7.659362e-11\n"]}],"source":["from copy import deepcopy\n","\n","from utils.gradient_check import eval_numerical_gradient\n","\n","X, y = init_toy_data()\n","\n","\n","def f(W):\n","    net.forward(X)\n","    return net.backward(y)\n","\n","for num in [2, 3]:\n","    net = init_toy_model(num)\n","    net.forward(X)\n","    net.backward(y)\n","    gradients = deepcopy(net.gradients)\n","\n","    for param_name in net.params:\n","        param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n","        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, gradients[param_name])))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}